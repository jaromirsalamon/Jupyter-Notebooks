{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['A real-time integrated data logistics and simple event processing platform Apache NiFi ' + \\\n",
    "        'automates the movement of data between disparate data sources and sy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for _, t in enumerate(text):\n",
    "    kw = []\n",
    "    parsed_text = nlp(t)\n",
    "    # taking all named entities as keywords\n",
    "    for entity in parsed_text.ents:\n",
    "        #kw.append(\"%s(%s)\" % (entity.text, entity.label_))\n",
    "        kw.append(entity.text)\n",
    "    for pt in parsed_text:\n",
    "        # taking just part of speech tags as keywords\n",
    "        if pt.dep_ in ('pobj', 'dobj', 'conj', 'compound') and pt.pos_ in ('NOUN') and pt.tag_ not in ('WP'):\n",
    "            #kw.append(\"%s(%s,%s)\"%(pt, pt.pos_, pt.tag_)) \n",
    "            kw.append(str(pt)) \n",
    "    # make the list of keyword unique when name entities and objects makes duplicity\n",
    "    kw = list(set(kw))\n",
    "    keywords.append(', '.join(str(k) for k in kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache NiFi, sources, movement, data, platform, event, processing']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see `NiFi` it self is missing, nobody will use `Apache Nifi` in conversation. So, let's train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train missing Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare training data containing keywords which we want to train the model and it's entity definitions by position and known NER keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    ('Apache NiFi (short for NiagaraFiles) is a software project from the Apache Software Foundation ' + \\\n",
    "        'designed to automate the flow of data between software systems.', {\n",
    "        'entities': [(0,6,'ORG'),(7,11,'PRODUCT')]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the built-in pipeline components and add them to the pipeline `nlp.create_pipe` works for built-ins that are registered with spaCy. Otherwise we need to get it, so we can add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add labels to the NER pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get names of other pipes to disable them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.4237863973823455}\n",
      "{'ner': 3.8326782247286757}\n",
      "{'ner': 3.3267226160073076}\n",
      "{'ner': 1.0775559683207607}\n",
      "{'ner': 2.9021405209103617}\n",
      "{'ner': 4.1294674029182659}\n",
      "{'ner': 4.3269160880255733}\n",
      "{'ner': 3.4417642318368848}\n",
      "{'ner': 3.0216557824286441}\n",
      "{'ner': 1.719915857021167}\n",
      "{'ner': 3.2125682612963056}\n",
      "{'ner': 1.4646328903200989}\n",
      "{'ner': 3.2772178058582129}\n",
      "{'ner': 0.86539458591505125}\n",
      "{'ner': 1.8290357937245514}\n",
      "{'ner': 1.752638645456996}\n",
      "{'ner': 1.4776314778831379}\n",
      "{'ner': 2.4013956641863157}\n",
      "{'ner': 1.9639375780212429}\n",
      "{'ner': 1.2426677611738506}\n"
     ]
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(20):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update(\n",
    "                [text],  # batch of texts\n",
    "                [annotations],  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Apache', 'ORG'), ('NiFi', 'PRODUCT')]\n",
      "Tokens [('Apache', 'ORG'), ('NiFi', 'PRODUCT'), ('(', ''), ('short', ''), ('for', ''), ('NiagaraFiles', ''), (')', ''), ('is', ''), ('a', ''), ('software', ''), ('project', ''), ('from', ''), ('the', ''), ('Apache', ''), ('Software', ''), ('Foundation', ''), ('designed', ''), ('to', ''), ('automate', ''), ('the', ''), ('flow', ''), ('of', ''), ('data', ''), ('between', ''), ('software', ''), ('systems', ''), ('.', '')]\n"
     ]
    }
   ],
   "source": [
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it recognizes now two new entities `Apache` as the organization and `NiFi` as the product.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
